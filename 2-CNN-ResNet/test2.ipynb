{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T06:20:12.254777Z",
     "start_time": "2025-03-18T06:19:46.064667Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# 🔹 MPS 사용 가능 여부 확인\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"✅ Using device: {device}\")\n",
    "\n",
    "# 데이터 로딩\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# CNN 모델 정의\n",
    "class BasicCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BasicCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 14 * 14, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = x.view(-1, 16 * 14 * 14)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# 모델 및 학습 설정\n",
    "model = BasicCNN().to(device)  # 모델을 GPU로 전송\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 🔹 CNN 모델 학습 (손실 함수 확인 + 모델 저장)\n",
    "num_epochs = 10  # 여러 번 학습 가능\n",
    "save_path = \"./mnist_cnn.pth\"\n",
    "\n",
    "print(\"🔄 모델 학습 시작...\")\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images, labels = images.to(device), labels.to(device)  # 데이터를 GPU로 전송\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # 100번마다 손실값 출력\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(trainloader)}], Loss: {running_loss / 100:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "# 학습 완료 후 모델 저장\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"✅ 모델이 저장되었습니다: {save_path}\")\n",
    "\n",
    "\n",
    "# 🔹 저장된 모델 불러오기\n",
    "def load_model(model_path):\n",
    "    model = BasicCNN()\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)  # 모델을 GPU로 전송\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 🔹 숫자 이미지 파일을 모델에 입력하여 예측하는 함수\n",
    "def predict_digit(image_path, model):\n",
    "    \"\"\"\n",
    "    사용자가 제공한 숫자 이미지 파일을 불러와 CNN 모델을 통해 예측하는 함수.\n",
    "    \"\"\"\n",
    "    # 이미지 불러오기 및 전처리\n",
    "    image = Image.open(image_path).convert(\"L\")  # 흑백 변환\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((28, 28)),  # MNIST 크기로 조정\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))  # 정규화\n",
    "    ])\n",
    "\n",
    "    image = transform(image).unsqueeze(0).to(device)  # 배치 차원 추가 및 GPU로 전송\n",
    "\n",
    "    # 모델 예측\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "\n",
    "    # 결과 출력\n",
    "    plt.imshow(image.cpu().squeeze(), cmap=\"gray\")  # 이미지를 CPU로 이동 후 시각화\n",
    "    plt.title(f\"Predicted Number: {predicted.item()}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"🔍 예측된 숫자: {predicted.item()}\")\n",
    "\n",
    "# ✅ 저장된 모델 불러오기\n",
    "loaded_model = load_model(\"./mnist_cnn.pth\")\n",
    "\n",
    "# 🏆 샘플 이미지 예측 실행 (예: \"digit.png\" 파일 입력)\n",
    "predict_digit(\"digit_0.png\", loaded_model)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps\n",
      "🔄 모델 학습 시작...\n",
      "Epoch [1/10], Step [100/938], Loss: 0.8771\n",
      "Epoch [1/10], Step [200/938], Loss: 0.3368\n",
      "Epoch [1/10], Step [300/938], Loss: 0.2804\n",
      "Epoch [1/10], Step [400/938], Loss: 0.2562\n",
      "Epoch [1/10], Step [500/938], Loss: 0.2054\n",
      "Epoch [1/10], Step [600/938], Loss: 0.2067\n",
      "Epoch [1/10], Step [700/938], Loss: 0.1881\n",
      "Epoch [1/10], Step [800/938], Loss: 0.1722\n",
      "Epoch [1/10], Step [900/938], Loss: 0.1619\n",
      "Epoch [2/10], Step [100/938], Loss: 0.1300\n",
      "Epoch [2/10], Step [200/938], Loss: 0.1327\n",
      "Epoch [2/10], Step [300/938], Loss: 0.1200\n",
      "Epoch [2/10], Step [400/938], Loss: 0.1227\n",
      "Epoch [2/10], Step [500/938], Loss: 0.1129\n",
      "Epoch [2/10], Step [600/938], Loss: 0.1155\n",
      "Epoch [2/10], Step [700/938], Loss: 0.0971\n",
      "Epoch [2/10], Step [800/938], Loss: 0.1111\n",
      "Epoch [2/10], Step [900/938], Loss: 0.1144\n",
      "Epoch [3/10], Step [100/938], Loss: 0.0890\n",
      "Epoch [3/10], Step [200/938], Loss: 0.0882\n",
      "Epoch [3/10], Step [300/938], Loss: 0.0943\n",
      "Epoch [3/10], Step [400/938], Loss: 0.0845\n",
      "Epoch [3/10], Step [500/938], Loss: 0.0811\n",
      "Epoch [3/10], Step [600/938], Loss: 0.0869\n",
      "Epoch [3/10], Step [700/938], Loss: 0.0805\n",
      "Epoch [3/10], Step [800/938], Loss: 0.0798\n",
      "Epoch [3/10], Step [900/938], Loss: 0.0710\n",
      "Epoch [4/10], Step [100/938], Loss: 0.0629\n",
      "Epoch [4/10], Step [200/938], Loss: 0.0698\n",
      "Epoch [4/10], Step [300/938], Loss: 0.0651\n",
      "Epoch [4/10], Step [400/938], Loss: 0.0762\n",
      "Epoch [4/10], Step [500/938], Loss: 0.0602\n",
      "Epoch [4/10], Step [600/938], Loss: 0.0707\n",
      "Epoch [4/10], Step [700/938], Loss: 0.0704\n",
      "Epoch [4/10], Step [800/938], Loss: 0.0642\n",
      "Epoch [4/10], Step [900/938], Loss: 0.0670\n",
      "Epoch [5/10], Step [100/938], Loss: 0.0540\n",
      "Epoch [5/10], Step [200/938], Loss: 0.0658\n",
      "Epoch [5/10], Step [300/938], Loss: 0.0597\n",
      "Epoch [5/10], Step [400/938], Loss: 0.0571\n",
      "Epoch [5/10], Step [500/938], Loss: 0.0671\n",
      "Epoch [5/10], Step [600/938], Loss: 0.0594\n",
      "Epoch [5/10], Step [700/938], Loss: 0.0604\n",
      "Epoch [5/10], Step [800/938], Loss: 0.0576\n",
      "Epoch [5/10], Step [900/938], Loss: 0.0541\n",
      "Epoch [6/10], Step [100/938], Loss: 0.0535\n",
      "Epoch [6/10], Step [200/938], Loss: 0.0497\n",
      "Epoch [6/10], Step [300/938], Loss: 0.0568\n",
      "Epoch [6/10], Step [400/938], Loss: 0.0420\n",
      "Epoch [6/10], Step [500/938], Loss: 0.0517\n",
      "Epoch [6/10], Step [600/938], Loss: 0.0563\n",
      "Epoch [6/10], Step [700/938], Loss: 0.0510\n",
      "Epoch [6/10], Step [800/938], Loss: 0.0588\n",
      "Epoch [6/10], Step [900/938], Loss: 0.0541\n",
      "Epoch [7/10], Step [100/938], Loss: 0.0456\n",
      "Epoch [7/10], Step [200/938], Loss: 0.0525\n",
      "Epoch [7/10], Step [300/938], Loss: 0.0420\n",
      "Epoch [7/10], Step [400/938], Loss: 0.0464\n",
      "Epoch [7/10], Step [500/938], Loss: 0.0495\n",
      "Epoch [7/10], Step [600/938], Loss: 0.0470\n",
      "Epoch [7/10], Step [700/938], Loss: 0.0461\n",
      "Epoch [7/10], Step [800/938], Loss: 0.0557\n",
      "Epoch [7/10], Step [900/938], Loss: 0.0431\n",
      "Epoch [8/10], Step [100/938], Loss: 0.0397\n",
      "Epoch [8/10], Step [200/938], Loss: 0.0397\n",
      "Epoch [8/10], Step [300/938], Loss: 0.0455\n",
      "Epoch [8/10], Step [400/938], Loss: 0.0450\n",
      "Epoch [8/10], Step [500/938], Loss: 0.0405\n",
      "Epoch [8/10], Step [600/938], Loss: 0.0435\n",
      "Epoch [8/10], Step [700/938], Loss: 0.0454\n",
      "Epoch [8/10], Step [800/938], Loss: 0.0442\n",
      "Epoch [8/10], Step [900/938], Loss: 0.0392\n",
      "Epoch [9/10], Step [100/938], Loss: 0.0325\n",
      "Epoch [9/10], Step [200/938], Loss: 0.0343\n",
      "Epoch [9/10], Step [300/938], Loss: 0.0402\n",
      "Epoch [9/10], Step [400/938], Loss: 0.0345\n",
      "Epoch [9/10], Step [500/938], Loss: 0.0404\n",
      "Epoch [9/10], Step [600/938], Loss: 0.0369\n",
      "Epoch [9/10], Step [700/938], Loss: 0.0375\n",
      "Epoch [9/10], Step [800/938], Loss: 0.0425\n",
      "Epoch [9/10], Step [900/938], Loss: 0.0459\n",
      "Epoch [10/10], Step [100/938], Loss: 0.0305\n",
      "Epoch [10/10], Step [200/938], Loss: 0.0286\n",
      "Epoch [10/10], Step [300/938], Loss: 0.0357\n",
      "Epoch [10/10], Step [400/938], Loss: 0.0385\n",
      "Epoch [10/10], Step [500/938], Loss: 0.0320\n",
      "Epoch [10/10], Step [600/938], Loss: 0.0425\n",
      "Epoch [10/10], Step [700/938], Loss: 0.0347\n",
      "Epoch [10/10], Step [800/938], Loss: 0.0374\n",
      "Epoch [10/10], Step [900/938], Loss: 0.0420\n",
      "✅ 모델이 저장되었습니다: ./mnist_cnn.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAGbCAYAAAAr/4yjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAEWpJREFUeJzt3Qus1nX9wPEvgnITEMUSMxHE8haJVGzecWoL81ZpailZrXSltWYabilKrZmWlrqUpaSoc0NrNqd42ai8dMFbadow8wIiYgsIb5Dw/Pf5bc+ncw7nAL/nTwcO5/XaTk885/k+z+/8jvzev8v3eejTaDQaBQBKKVtt6gUAYPMhCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCrRst912K1/4whfyz7/5zW9Knz59qtvNdRl7ovgZPvnJT27qxaCXEIUe6he/+EW1AW5+DRgwoHzgAx8oX//618trr71WepK77767TJs2bZMuQ3M9/uhHP+pyXT/66KObZNk2d48//ng59thjy/bbb18GDRpU9t133/LTn/50Uy8WLerX6kA2D5dcckkZPXp0eeedd8pDDz1Ufvazn1Ub2aeffrr6C9qdDjnkkPL222+XbbbZpta4WN5rrrlmk4chXHbZZeWss87q9nXXU913333lmGOOKePHjy/f/e53y7bbbluef/75snDhwk29aLRIFHq4T3ziE+UjH/lI9f+//OUvlx122KH8+Mc/LnfeeWc55ZRTOh3z5ptvlsGDB2/0Zdlqq62qI5aear/99itPPvlkufbaa8u3vvWt0pu8++67Zc2aNbWC/u9//7ucfvrp5eijjy6333579fun5/Nb3MIcfvjh1e0LL7xQ3cb59Obe2+TJk8uQIUPK5z73uep7sRG48soryz777FNtzN/73veWr371q2Xp0qXtnjM+SPd73/te2WWXXao96EmTJpW//vWva712V9cU/vjHP1avPXz48CpG48aNKz/5yU9y+eIoIbQ9Hda0sZdxXQ488MBq/f3whz+sjnjW5bDDDqu+OoqfJ64BNL344ovVz3P55ZdXP+eYMWOq5TvqqKPKggULquWePn16tdwDBw4sxx13XPnXv/7V5V55hCvWw957711++ctfrvWYZcuWlW9+85vl/e9/f+nfv38ZO3ZsufTSS6v12Nkyxbrdfffdq8c+88wz1ff/9re/lZdffnm96+vWW2+tTlV+//vfr4IQOxttX4eeyZHCFiY2/iGOGNruBX784x8vBx10ULUhaJ4aiY1rnC8/44wzyjnnnFOF5Oqrry5PPPFEefjhh8vWW29dPe7CCy+sNrixYY+vOIccG7VVq1atd3nuv//+6iLpyJEjyze+8Y2y0047lWeffbbcdddd1Z9jGRYtWlQ9btasWWuN745lbCtOYcVpsDgNtzGPFm655ZZqWc4+++xqox/hOemkk6oIRUTPP//88ve//71cddVV5dxzzy033HBDu/HPPfdc+exnP1vOPPPMMmXKlDJz5sxy4oknljlz5pQjjzyyesxbb71VDj300PLKK69U623XXXctjzzySJk6dWp59dVXqwC0Fc8Rpx2/8pWvVFGIawJhr732qp5nfRMGHnjggTJ06NDq9Y4//vgyf/78KvqnnXZaueKKK3r0UWOvFv+eAj3PzJkz49/BaDzwwAON119/vbFgwYLGbbfd1thhhx0aAwcObCxcuLB63JQpU6rHfec732k3/sEHH6zuv+WWW9rdP2fOnHb3L1mypLHNNts0jj766MaaNWvycRdccEH1uHj+prlz51b3xW149913G6NHj26MGjWqsXTp0nav0/a5vva1r1XjOvpfLGNX4nGxHGHSpEmNnXbaqfHWW2+1W9fz5s3Lxx966KHVV0fxWvHzNr3wwgvV2B133LGxbNmyvH/q1KnV/R/+8Icb//nPf/L+U045pfpZ3nnnnbwvni8ee8cdd+R9y5cvb4wcObIxfvz4vG/69OmNwYMHN+bPn99umeJ337dv38bLL7/cbpmGDh1arbvO1kVnP1tH48aNawwaNKj6Ovvss6vli9sYf/LJJ693PJsnp496uCOOOKLsuOOO1emCk08+uTpV9Ktf/aq8733va/e4uHja1uzZs8uwYcOqvcx//vOf+TVhwoTqOebOnZt7g8093LandeIUxfrE3nzs2cdjt9tuu3bfa/tcXemOZezqaGHx4sXVtYWNJfbq42dpmjhxYnX7+c9/vvTr16/d/fGzxN53WzvvvHM54YQT8s+xhx7n82Mdx7I219fBBx9cnaZru77iv5HVq1eX3/3ud+2e89Of/nT1305H0YUNmVb8xhtvVEcnsRwx2+hTn/pUdRtHKbfddlt1dEPP4/RRDxfnqWMqamxY4nz7Bz/4wbUu+MX34px1W/EXdvny5eU973lPp8+7ZMmS6vall16qbvfYY49234+NSWx8NuRUVkxRbEV3LGNn4vRRXJOIUzxxumZjiFM5bTUDETHv7P6O10zi2kDHkMbvvXmNIE7Lxfr6y1/+0umGvu36aopZa/8fcQ0kdJzQcOqpp5brrruu/P73v1/rd8LmTxR6uI997GM5+6grcb64YyjigmBsbONcd2e62rB0p025jBdddFF1ITk2bh2PckJsoDv7l2xjj7wzffv2rXV/K/9KbqyvOKo677zzOv1+MyIdN+qtiqOXuJgfOyNtNSPeMWz0DKLQS8WMkzjtEjNu1rVxGDVqVHUbe6Exc6bp9ddfX+9f+niNEO+ZiFMYXenqVFJ3LGNX4kJrRCFm7sRF7I7iCOQf//jHWvc3j1o2trgIHaFou67iwm5oznaK9RWndNa1rjemOI0XEwTiVFccoTbFxIHNZceC+lxT6KVi5kvs1cZ0yI5itlJMbQyxgYkZPjErpu3ea8eZLJ3Zf//9q1MU8djm8zW1fa7meyY6PqY7lnFDri3MmDFjre/FBjimbkZ4mv785z9XM6L+F2JDG9eK2r5H4KabbqqmqMapo+b6ilM2995771rjY13FOtsQGzolNV4vXH/99e3u//nPf16dsuxsyi6bP0cKvVTsCccFwR/84AfVG7Zi+mZsWGNvOy5YxvsIPvOZz1R7ezFFMh4XU0tjumdc3LznnnvKiBEj1vkaccoqpnbGO15j4xXTSmNqamx04rRDc+MVe5whppzG1Nk4pRIXzbtjGde3juLrt7/97Vrf++IXv1i9STCW90tf+lJ1vj4uTMf7KWKDvbHFqZ94nXnz5lWna2LKarxHIKaVNn37298uv/71r6t1EO+XiPUa7x146qmnqjeXxbWHDVkfGzolNd7FHOshliWC0xwTv5uYBhunl+iBNvX0J1rT2TTJzsQUyZim2JUZM2Y0JkyYUE1jHTJkSONDH/pQ47zzzmssWrQoH7N69erGxRdfXE2BjMcddthhjaeffrqaKrmuKalNDz30UOPII4+snj+WJaYyXnXVVfn9mLoaUxlj2mafPn3Wmp66MZdxQ6akttX8mTpb1zfffHNjzJgx1RTS/fbbr3Hvvfd2OSX1sssu6/R5Z8+evd7fazxfTLeN5491179//8aee+651tiwYsWKarrr2LFjq+UaMWJE44ADDmhcfvnljVWrVq1zmepOSQ3xnNOmTauWceutt65e94orrtigsWye+sT/bOowAbB5cE0BgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKAKR+//2/sOVYunRp7THLly+vPWb16tW1xwwePLi0YtiwYbXHDBw4sKXXovdypABAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACSfksoW6Zprrqk95vrrr689ZsGCBbXHTJ48ubTi3HPPrT3mkEMOaem16L0cKQCQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIPlAPLrNqlWrWho3f/782mOef/752mP22muv2mOmTJlSe8ysWbNKK5599tnaY8aPH197zJAhQ2qPYcvhSAGAJAoAJFEAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAMkH4tFtVq5c2dK4efPm1R7Tt2/f2mOOOOKI2mNOOumk2mPmzJlTWrF48eLaY1555ZXaY/bcc8/aY9hyOFIAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDygXhs9h+I99hjj9UeM2LEiNpj9t5779pj+vfvX3vMxIkTSyvefvvt2mNefPHF2mN8IF7v5kgBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIPiWVbrN69eqWxi1btqz2mOHDh9ceM2jQoNpj+vbtW3vM0KFDSyvefPPN2mNWrFjR0mvRezlSACCJAgBJFABIogBAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBA8oF4dJuttmptH2Tw4MHd8uF7K1eurD1mzZo13fLBdqFfv/p/XQcMGNDSa9F7OVIAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEDygXh0m1Y+0C2MHTu29piXXnqp9piFCxfWHrPPPvvUHvPcc8+VVowfP772mJEjR7b0WvRejhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJB8IB7dZsCAAS2NmzhxYu0xTzzxRO0xf/jDH2qP2W233WqPefzxx0srJk2aVHvMmDFjWnotei9HCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASD4Qj83+A/E++tGP1h4ze/bs2mNuvPHG2mNmzJhRe8y4ceNKK/bYY4/aY4YNG9bSa9F7OVIAIIkCAEkUAEiiAEASBQCSKACQRAGAJAoAJFEAIIkCAEkUAEiiAEASBQBSn0aj0fjvH2HL8Kc//an2mGeeeab2mDfeeKP2mFGjRpVWTJgwofaYnXfeuaXXovdypABAEgUAkigAkEQBgCQKACRRACCJAgBJFABIogBAEgUAkigAkEQBgOQD8QBIjhQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYAkCgAkUQAgiQIASRQASKIAQBIFAJIoAJBEAYDS9H9GqLWKx6ShlwAAAABJRU5ErkJggg=="
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 예측된 숫자: 6\n"
     ]
    }
   ],
   "execution_count": 3
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
