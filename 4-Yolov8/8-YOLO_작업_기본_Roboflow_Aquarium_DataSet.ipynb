{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T02:29:15.883510Z",
     "start_time": "2025-03-24T02:29:13.519746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from ultralytics import YOLO\n",
    "import zipfile\n",
    "import cv2\n",
    "\n",
    "# 작업 경로 설정 (로컬 경로)\n",
    "base_path = \"/Users/ihanjo/Documents/K-Digital/deepAI_Pytorch_my/4-Yolov8/YOLOv8\"\n",
    "os.makedirs(base_path, exist_ok=True)\n"
   ],
   "id": "f68ea0c3fcae5f6d",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# ultralytics 설치 (터미널에서 실행 필요)\n",
    "# pip install ultralytics\n",
    "\n",
    "# YOLO 설치 확인\n",
    "# YOLO.checks()\n"
   ],
   "id": "7a46d3e9a4aa0127",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 데이터셋 다운로드 및 압축 해제\n",
    "dataset_url = \"https://public.roboflow.com/ds/EUBO4ATjbt?key=dnYrjDbMEB\"\n",
    "zip_path = os.path.join(base_path, \"Aquarium_DataSet.zip\")\n",
    "extract_path = os.path.join(base_path, \"Aquarium_DataSet\")\n",
    "\n",
    "# wget 대신 urllib 사용\n",
    "import urllib.request\n",
    "urllib.request.urlretrieve(dataset_url, zip_path)\n",
    "\n",
    "if not os.path.exists(extract_path):\n",
    "    os.makedirs(extract_path)\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_path)\n",
    "\n",
    "print(f\"Aquarium_DataSet 압축 해제가 완료되었습니다. 폴더 경로: {extract_path}\")\n"
   ],
   "id": "5733f3ddf98e9730",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# YAML 파일 생성\n",
    "yaml_path = os.path.join(extract_path, \"Aquarium_Data.yaml\")\n",
    "data = {\n",
    "    'train': os.path.join(extract_path, \"train\", \"images\") + \"/\",\n",
    "    'val': os.path.join(extract_path, \"valid\", \"images\") + \"/\",\n",
    "    'test': os.path.join(extract_path, \"test\", \"images\") + \"/\",\n",
    "    'nc': 7,\n",
    "    'names': ['fish', 'jellyfish', 'penguin', 'puffin', 'shark', 'starfish', 'stingray']\n",
    "}\n",
    "\n",
    "import yaml\n",
    "with open(yaml_path, 'w') as f:\n",
    "    yaml.dump(data, f)\n",
    "\n",
    "print(\"YAML 파일 생성 완료:\", yaml_path)\n"
   ],
   "id": "8d249cd362f37451",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 사전 학습된 YOLO 모델 로드\n",
    "model = YOLO(\"yolov8n.pt\")  # 사전 학습된 모델 로드\n",
    "\n",
    "# 모델 학습\n",
    "results = model.train(\n",
    "    data=yaml_path,         # 데이터셋 구성 파일 경로\n",
    "    epochs=50,             # 학습 에포크 수\n",
    "    patience=30,            # 조기 종료 기준 에포크 수\n",
    "    batch=64,               # 배치 크기\n",
    "    imgsz=416,              # 입력 이미지 크기\n",
    "    project=base_path,      # 결과 저장 경로\n",
    "    name='modelresult',     # 프로젝트 이름\n",
    "    device=\"mps\"            # Apple Silicon GPU(MPS) 사용\n",
    ")\n",
    "\n",
    "print(f\"모델 학습 완료. 저장 경로: {results.save_dir}\")\n"
   ],
   "id": "1ca4da88e854c1e4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T02:29:19.575976Z",
     "start_time": "2025-03-24T02:29:18.773186Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 예측 실행 (이미지)\n",
    "model = YOLO(os.path.join(base_path, \"modelresult\", \"best.pt\"))\n",
    "\n",
    "test_image = os.path.join(extract_path, \"test\", \"images\", \"IMG_2301_jpeg_jpg.rf.2c19ae5efbd1f8611b5578125f001695.jpg\")\n",
    "output_dir = os.path.join(base_path, \"modelresultImg/\")\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "results = model.predict(\n",
    "    source=test_image,\n",
    "    save=True,\n",
    "    save_dir=output_dir\n",
    ")\n",
    "\n",
    "print(f\"결과 이미지가 다음 경로에 저장되었습니다: {output_dir}\")\n"
   ],
   "id": "e9c2e4d51c111d01",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/ihanjo/Documents/K-Digital/deepAI_Pytorch_my/4-Yolov8/YOLOv8/modelresult/best.pt'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# 예측 실행 (이미지)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mYOLO\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodelresult\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbest.pt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      4\u001B[0m test_image \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(extract_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIMG_2301_jpeg_jpg.rf.2c19ae5efbd1f8611b5578125f001695.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      5\u001B[0m output_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(base_path, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodelresultImg/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/ultralytics/models/yolo/model.py:23\u001B[0m, in \u001B[0;36mYOLO.__init__\u001B[0;34m(self, model, task, verbose)\u001B[0m\n\u001B[1;32m     20\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m \u001B[38;5;241m=\u001B[39m new_instance\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__dict__\u001B[39m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     22\u001B[0m     \u001B[38;5;66;03m# Continue with default YOLO initialization\u001B[39;00m\n\u001B[0;32m---> 23\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverbose\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/ultralytics/engine/model.py:148\u001B[0m, in \u001B[0;36mModel.__init__\u001B[0;34m(self, model, task, verbose)\u001B[0m\n\u001B[1;32m    146\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(model, task\u001B[38;5;241m=\u001B[39mtask, verbose\u001B[38;5;241m=\u001B[39mverbose)\n\u001B[1;32m    147\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 148\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;66;03m# Delete super().training for accessing self.model.training\u001B[39;00m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/ultralytics/engine/model.py:290\u001B[0m, in \u001B[0;36mModel._load\u001B[0;34m(self, weights, task)\u001B[0m\n\u001B[1;32m    287\u001B[0m weights \u001B[38;5;241m=\u001B[39m checks\u001B[38;5;241m.\u001B[39mcheck_model_file_from_stem(weights)  \u001B[38;5;66;03m# add suffix, i.e. yolo11n -> yolo11n.pt\u001B[39;00m\n\u001B[1;32m    289\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m Path(weights)\u001B[38;5;241m.\u001B[39msuffix \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m.pt\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 290\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mckpt \u001B[38;5;241m=\u001B[39m \u001B[43mattempt_load_one_weight\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweights\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    291\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtask\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moverrides \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset_ckpt_args(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel\u001B[38;5;241m.\u001B[39margs)\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/ultralytics/nn/tasks.py:1039\u001B[0m, in \u001B[0;36mattempt_load_one_weight\u001B[0;34m(weight, device, inplace, fuse)\u001B[0m\n\u001B[1;32m   1026\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mattempt_load_one_weight\u001B[39m(weight, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, fuse\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m   1027\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1028\u001B[0m \u001B[38;5;124;03m    Load a single model weights.\u001B[39;00m\n\u001B[1;32m   1029\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1037\u001B[0m \u001B[38;5;124;03m        (tuple): Tuple containing the model and checkpoint.\u001B[39;00m\n\u001B[1;32m   1038\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 1039\u001B[0m     ckpt, weight \u001B[38;5;241m=\u001B[39m \u001B[43mtorch_safe_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# load ckpt\u001B[39;00m\n\u001B[1;32m   1040\u001B[0m     args \u001B[38;5;241m=\u001B[39m {\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mDEFAULT_CFG_DICT, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(ckpt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_args\u001B[39m\u001B[38;5;124m\"\u001B[39m, {}))}  \u001B[38;5;66;03m# combine model and default args, preferring model args\u001B[39;00m\n\u001B[1;32m   1041\u001B[0m     model \u001B[38;5;241m=\u001B[39m (ckpt\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mema\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mor\u001B[39;00m ckpt[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodel\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mto(device)\u001B[38;5;241m.\u001B[39mfloat()  \u001B[38;5;66;03m# FP32 model\u001B[39;00m\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/ultralytics/nn/tasks.py:944\u001B[0m, in \u001B[0;36mtorch_safe_load\u001B[0;34m(weight, safe_only)\u001B[0m\n\u001B[1;32m    942\u001B[0m                 ckpt \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mload(f, pickle_module\u001B[38;5;241m=\u001B[39msafe_pickle)\n\u001B[1;32m    943\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 944\u001B[0m             ckpt \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# e.name is missing module name\u001B[39;00m\n\u001B[1;32m    947\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m e\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmodels\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/ultralytics/utils/patches.py:86\u001B[0m, in \u001B[0;36mtorch_load\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TORCH_1_13 \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweights_only\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[1;32m     84\u001B[0m     kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mweights_only\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m---> 86\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_torch_load\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/torch/serialization.py:1425\u001B[0m, in \u001B[0;36mload\u001B[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001B[0m\n\u001B[1;32m   1422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m pickle_load_args\u001B[38;5;241m.\u001B[39mkeys():\n\u001B[1;32m   1423\u001B[0m     pickle_load_args[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mencoding\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 1425\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[43m_open_file_like\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m opened_file:\n\u001B[1;32m   1426\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_zipfile(opened_file):\n\u001B[1;32m   1427\u001B[0m         \u001B[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001B[39;00m\n\u001B[1;32m   1428\u001B[0m         \u001B[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001B[39;00m\n\u001B[1;32m   1429\u001B[0m         \u001B[38;5;66;03m# reset back to the original position.\u001B[39;00m\n\u001B[1;32m   1430\u001B[0m         orig_position \u001B[38;5;241m=\u001B[39m opened_file\u001B[38;5;241m.\u001B[39mtell()\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/torch/serialization.py:751\u001B[0m, in \u001B[0;36m_open_file_like\u001B[0;34m(name_or_buffer, mode)\u001B[0m\n\u001B[1;32m    749\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_open_file_like\u001B[39m(name_or_buffer, mode):\n\u001B[1;32m    750\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m _is_path(name_or_buffer):\n\u001B[0;32m--> 751\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_open_file\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname_or_buffer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    752\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    753\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mw\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m mode:\n",
      "File \u001B[0;32m~/Library/Python/3.13/lib/python/site-packages/torch/serialization.py:732\u001B[0m, in \u001B[0;36m_open_file.__init__\u001B[0;34m(self, name, mode)\u001B[0m\n\u001B[1;32m    731\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, name, mode):\n\u001B[0;32m--> 732\u001B[0m     \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/Users/ihanjo/Documents/K-Digital/deepAI_Pytorch_my/4-Yolov8/YOLOv8/modelresult/best.pt'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 비디오 예측 실행\n",
    "video_path = \"/Users/your_username/Documents/YOLOTest7/sample_video.mp4\"\n",
    "output_video = os.path.join(base_path, \"output_video.mp4\")\n",
    "\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Cannot open video file.\")\n",
    "    exit()\n",
    "\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "\n",
    "out = cv2.VideoWriter(output_video, fourcc, fps, (width, height))\n",
    "\n",
    "frame_count = 0\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    results = model.predict(source=frame, save=False, conf=0.5)\n",
    "    annotated_frame = results[0].plot()\n",
    "    out.write(annotated_frame)\n",
    "\n",
    "cap.release()\n",
    "out.release()\n",
    "print(f\"Detection completed. Output video saved at {output_video}\")\n"
   ],
   "id": "eacd27c9282c16e3",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
